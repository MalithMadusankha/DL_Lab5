{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/kameshDiviyanjana/DL-Lab-5/blob/main/IT21155352_Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"V1azvxjVeI6m"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vCyMiwtcbud","executionInfo":{"status":"ok","timestamp":1725175534591,"user_tz":-330,"elapsed":25918,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}},"outputId":"fe58bcef-4d3e-419c-bd7d-0afad4051f67"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"KK6KEwUIU9G9","executionInfo":{"status":"ok","timestamp":1725175538693,"user_tz":-330,"elapsed":4104,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.model_selection import train_test_split\n","import re"]},{"cell_type":"code","source":["# 1. Load and Preprocess the Dataset\n","def load_data(file_path):\n","    # Load the dataset (e.g., IMDB movie reviews dataset)\n","    df = pd.read_csv(file_path, engine='python', on_bad_lines='skip')  # Using 'python' engine and skipping bad lines\n","    df.dropna(inplace=True)  # Drop any rows with missing values\n","    return df['review'], df['sentiment']  # Assuming 'review' and 'sentiment' columns"],"metadata":{"id":"cSk83Rz1Xcv8","executionInfo":{"status":"ok","timestamp":1725175538694,"user_tz":-330,"elapsed":12,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Clean the text\n","def clean_text(text):\n","    # Remove unwanted characters, numbers, and symbols\n","    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text"],"metadata":{"id":"IOq6ArhtXrOp","executionInfo":{"status":"ok","timestamp":1725175538694,"user_tz":-330,"elapsed":11,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Tokenize and Pad Sequences\n","def preprocess_text(reviews, max_words=5000, max_len=200):\n","    reviews = [clean_text(review) for review in reviews]  # Clean the reviews\n","    tokenizer = Tokenizer(num_words=max_words)\n","    tokenizer.fit_on_texts(reviews)\n","    sequences = tokenizer.texts_to_sequences(reviews)\n","    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n","    return padded_sequences, tokenizer"],"metadata":{"id":"SVW33pLHXuy0","executionInfo":{"status":"ok","timestamp":1725175538695,"user_tz":-330,"elapsed":12,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Encode Sentiments\n","def encode_labels(sentiments):\n","    sentiments = sentiments.map({'positive': 1, 'negative': 0}).values\n","    return sentiments"],"metadata":{"id":"3KIzYbQAXu1O","executionInfo":{"status":"ok","timestamp":1725175538695,"user_tz":-330,"elapsed":12,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Load Data\n","file_path = '/content/drive/MyDrive/SLIIT/DL/LAB5/IMDBDataset.csv'  # <-- Provide the correct path to the dataset\n","reviews, sentiments = load_data(file_path)"],"metadata":{"id":"fXk9eooIXu3c","executionInfo":{"status":"ok","timestamp":1725175542778,"user_tz":-330,"elapsed":4094,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Preprocess Text Data\n","max_words = 6000  # Consider the top 5000 words\n","max_len = 200  # Pad or truncate reviews to 200 words\n","X, tokenizer = preprocess_text(reviews, max_words=max_words, max_len=max_len)"],"metadata":{"id":"Oz_KVFMtXu51","executionInfo":{"status":"ok","timestamp":1725175562074,"user_tz":-330,"elapsed":19306,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Encode Sentiments (positive -> 1, negative -> 0)\n","y = encode_labels(sentiments)\n","\n","# Split into Training and Testing Sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"Xz6LCQ2OX4Zc","executionInfo":{"status":"ok","timestamp":1725175562075,"user_tz":-330,"elapsed":4,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 2. Define the LSTM Model\n","model = Sequential()\n","\n","# Modify the embedding dimensions and experiment with LSTM configurations ---\n","model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))  # <-- Modify 'output_dim'\n","model.add(Bidirectional(LSTM(units=74, return_sequences=False)))  # <-- Experiment with 'units' and add Dropout if necessary\n","\n","model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# 3. Train the Model\n","#  Modify 'epochs' and 'batch_size' to see how they impact training time and model accuracy ---\n","model.fit(X_train, y_train, epochs=5, batch_size=52, validation_data=(X_test, y_test), verbose=1)  # <-- Experiment with 'epochs' and 'batch_size'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9VE7zaVX4c1","outputId":"c90e4b27-c4ce-4b53-dbe0-ae15e87f4421","executionInfo":{"status":"ok","timestamp":1725175656511,"user_tz":-330,"elapsed":94440,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.7345 - loss: 0.5111 - val_accuracy: 0.8722 - val_loss: 0.3172\n","Epoch 2/5\n","\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8893 - loss: 0.2781 - val_accuracy: 0.8860 - val_loss: 0.2817\n","Epoch 3/5\n","\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9159 - loss: 0.2153 - val_accuracy: 0.8776 - val_loss: 0.3094\n","Epoch 4/5\n","\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9331 - loss: 0.1769 - val_accuracy: 0.8870 - val_loss: 0.3019\n","Epoch 5/5\n","\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9523 - loss: 0.1320 - val_accuracy: 0.8831 - val_loss: 0.3422\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7dbf2cdc4b80>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# 4. Evaluate the Model\n","y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")"],"metadata":{"id":"y5xUa5tmX4fx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725175659001,"user_tz":-330,"elapsed":2495,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}},"outputId":"7293bfb8-da80-4e41-8bc5-dbd4ebccc23e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n"]}]},{"cell_type":"code","source":["# Calculate Accuracy and F1-Score\n","accuracy = accuracy_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(f'Accuracy: {accuracy:.4f}')\n","print(f'F1-Score: {f1:.4f}')"],"metadata":{"id":"FW2M5tWTX_bc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725175659001,"user_tz":-330,"elapsed":3,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}},"outputId":"c3c3e4f7-16a1-4db8-dabc-9c6e7a3b5cc2"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8831\n","F1-Score: 0.8820\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tymAeIP2YBqZ","executionInfo":{"status":"ok","timestamp":1725175659001,"user_tz":-330,"elapsed":3,"user":{"displayName":"Malith Madusankha","userId":"03551031913765327076"}}},"execution_count":14,"outputs":[]}]}